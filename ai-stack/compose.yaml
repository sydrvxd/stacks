# AI Stack for Local LLM & Whisper Inference
# Hardware: Intel i5-14500, RTX 5070 Ti (16GB), 64GB DDR5
#
# Components:
# - Ollama: LLM inference on RTX 5070 Ti
# - Open WebUI: Chat interface for Ollama
# - Faster-Whisper API: Speech-to-text (CPU/iGPU to keep GPU free for LLMs)
# - LiteLLM Proxy: OpenAI-compatible API for VS Code/external tools
#
# Use Cases:
# 1. Open WebUI for chat
# 2. Whisper API for subtitle transcription + LLM translation
# 3. OpenAI-compatible API for VS Code (Continue, Cody, etc.)

services:
  # =============================================================================
  # OLLAMA - LLM Inference Server (GPU)
  # =============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped

    # GPU passthrough for RTX 5070 Ti
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute]

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      # Optimize for 16GB VRAM
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
      # Flash attention for better performance
      - OLLAMA_FLASH_ATTENTION=1

    volumes:
      - ollama-data:/root/.ollama

    ports:
      - "${OLLAMA_PORT:-11434}:11434"

    # Resource limits
    mem_limit: 32g
    cpus: 8

    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 20s

  # =============================================================================
  # OPEN WEBUI - Chat Interface
  # =============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped

    depends_on:
      ollama:
        condition: service_healthy

    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-changeme-secret-key}
      # Enable RAG features
      - ENABLE_RAG_WEB_SEARCH=${ENABLE_RAG_WEB_SEARCH:-false}
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      # Whisper integration
      - AUDIO_STT_ENGINE=openai
      - AUDIO_STT_OPENAI_API_BASE_URL=http://whisper-api:8000/v1
      - AUDIO_STT_OPENAI_API_KEY=not-needed
      - AUDIO_STT_MODEL=whisper-large-v3
      # TTS (optional, uses browser TTS by default)
      - AUDIO_TTS_ENGINE=openai
      - AUDIO_TTS_OPENAI_API_BASE_URL=${TTS_API_URL:-}
      - AUDIO_TTS_MODEL=${TTS_MODEL:-}

    volumes:
      - open-webui-data:/app/backend/data

    ports:
      - "${WEBUI_PORT:-3000}:8080"

    mem_limit: 4g
    cpus: 4

  # =============================================================================
  # FASTER-WHISPER API - Speech-to-Text (CPU to keep GPU for LLMs)
  # =============================================================================
  whisper-api:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: whisper-api
    restart: unless-stopped

    environment:
      # Use CPU to keep GPU free for LLMs
      # Intel i5-14500 has strong AVX-512 support
      - WHISPER__MODEL=large-v3
      - WHISPER__DEVICE=cpu
      - WHISPER__COMPUTE_TYPE=int8
      # Optimize for 64GB RAM - can load large models
      - WHISPER__CPU_THREADS=8
      # Language settings
      - WHISPER__DEFAULT_LANGUAGE=de
      # API settings
      - UVICORN_HOST=0.0.0.0
      - UVICORN_PORT=8000

    volumes:
      - whisper-models:/root/.cache/huggingface
      # Mount for file processing
      - whisper-tmp:/tmp/whisper

    ports:
      - "${WHISPER_PORT:-8000}:8000"

    # CPU resources for Whisper
    mem_limit: 16g
    cpus: 10

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # =============================================================================
  # LITELLM PROXY - OpenAI-Compatible API Gateway
  # =============================================================================
  # Provides unified OpenAI-compatible API for:
  # - VS Code extensions (Continue, Cody, Copilot alternatives)
  # - AutoUkSubtitle and other apps expecting OpenAI API
  # - Any tool that supports OpenAI API format
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    restart: unless-stopped

    depends_on:
      ollama:
        condition: service_healthy

    environment:
      - LITELLM_MASTER_KEY=${LITELLM_API_KEY:-sk-local-dev-key}
      - LITELLM_LOG=DEBUG

    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro

    command: ["--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0"]

    ports:
      - "${LITELLM_PORT:-4000}:4000"

    mem_limit: 2g
    cpus: 2

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # =============================================================================
  # OPTIONAL: Whisper with GPU (uncomment if you want GPU acceleration)
  # =============================================================================
  # whisper-api-gpu:
  #   image: fedirz/faster-whisper-server:latest-cuda
  #   container_name: whisper-api-gpu
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu, compute]
  #   environment:
  #     - WHISPER__MODEL=large-v3
  #     - WHISPER__DEVICE=cuda
  #     - WHISPER__COMPUTE_TYPE=float16
  #     - WHISPER__DEFAULT_LANGUAGE=de
  #   volumes:
  #     - whisper-models:/root/.cache/huggingface
  #   ports:
  #     - "8001:8000"

volumes:
  ollama-data:
    name: ollama-data
  open-webui-data:
    name: open-webui-data
  whisper-models:
    name: whisper-models
  whisper-tmp:
    name: whisper-tmp

networks:
  default:
    name: ai-stack-network
