# =============================================================================
# Auto UK Subtitle - Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Application Source Path (for Docker build)
# -----------------------------------------------------------------------------
# Path to the AutoUkSubtitle_WebService project
APP_SOURCE_PATH=/path/to/AutoUkSubtitle_WebService

# -----------------------------------------------------------------------------
# Port Configuration
# -----------------------------------------------------------------------------
WEB_PORT=8085

# -----------------------------------------------------------------------------
# Media Path
# -----------------------------------------------------------------------------
# Root directory containing your video files
# This will be mounted read-only in the container
MEDIA_PATH=/mnt/qnap/Multimedia

# -----------------------------------------------------------------------------
# Debug & Logging
# -----------------------------------------------------------------------------
DEBUG=false
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Whisper API Configuration
# -----------------------------------------------------------------------------
# URL of the Faster-Whisper API (from ai-stack)
WHISPER_API_URL=http://whisper-api:8000
WHISPER_MODEL=large-v3
WHISPER_LANGUAGE=de

# -----------------------------------------------------------------------------
# LLM API Configuration
# -----------------------------------------------------------------------------
# URL of the Ollama API (OpenAI-compatible endpoint)
LLM_API_URL=http://ollama:11434/v1
OLLAMA_API_URL=http://ollama:11434
LLM_API_KEY=ollama

# Translation model - gpt-oss:20b recommended for best DE->UK translation
# gpt-oss:20b has 128K context, excellent multilingual capabilities
# Alternatives: gemma3:12b (8K), aya-expanse:8b (8K), llama3.1:8b (128K)
LLM_MODEL=gpt-oss:20b
LLM_TRANSLATION_MODEL=gpt-oss:20b
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=16384

# Reasoning effort for gpt-oss models (low=fast, medium=balanced, high=detailed)
LLM_REASONING_EFFORT=medium

# -----------------------------------------------------------------------------
# Translation Settings (Optimized for gpt-oss:20b)
# -----------------------------------------------------------------------------
# Maximum characters per translation chunk (calculated dynamically if possible)
# For gpt-oss:20b with 128K context, optimal is 15000-20000 chars
TRANSLATION_CHUNK_SIZE=15000

# Retry settings (increased for self-healing workflow)
TRANSLATION_MAX_RETRIES=7
TRANSLATION_TIMEOUT=600

# Chunk overlap (subtitle blocks to overlap for context)
TRANSLATION_CHUNK_OVERLAP=2

# -----------------------------------------------------------------------------
# Self-Healing / Circuit Breaker Settings
# -----------------------------------------------------------------------------
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=60
HEALTH_CHECK_INTERVAL=30

# -----------------------------------------------------------------------------
# Processing Settings
# -----------------------------------------------------------------------------
# Maximum parallel processing jobs
MAX_PARALLEL_JOBS=2

# Enable spot-check verification for existing subtitles
SPOT_CHECK_ENABLED=true

# -----------------------------------------------------------------------------
# Redis Configuration
# -----------------------------------------------------------------------------
REDIS_URL=redis://redis:6379/0
